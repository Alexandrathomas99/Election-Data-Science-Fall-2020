---
title: "Data Acquisition Tutorial"
subtitle: "Election Data Science"  
author: 
  - "Peter Licari"
date: '`r Sys.Date()`'
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_duo_accent(
  primary_color = "#1381B0",
  secondary_color = "#FF961C",
  inverse_header_color = "#FFFFFF",
  text_font_google = google_font("Poppins")
)

xaringanExtra::use_xaringan_extra()
library(tidyverse)
library(kableExtra)
```
#What we'll be covering

--

1. How to read-in a csv
2. How to read-in a tsv
3. How to read-in a xlsx
4. How to acquire data from the census
5. How to download data from site directly
6. How to read-in data from a web (REST) API
7. How to scrape a table from Wikipedia



---
# Set up

You can get the data for today at the class GitHub site. (Don't forget to set your working directory!)

```{r}
library(tidyverse)
library(readxl)
library(httr)
library(rvest)
```



---
# Reading in a csv

.panelset[
.panel[.panel-name[The Code]

```{r}
#Make sure you have readr and/or tidyverse active!

Pres_Cands_2020 <- read_csv("fec_cands_july.csv")
```
]

.panel[.panel-name[Output]

```{r echo=FALSE}
kbl(head(Pres_Cands_2020)) %>%
    kable_paper() %>%
  scroll_box(width = "800px", height = "350px")

```
]
]

---
# Reading in a tsv

.panelset[
.panel[.panel-name[The Code]

```{r message=FALSE, warning=FALSE}
#Remember, sometimes TSVs (and CSVs--and PSVs for that matter) are written as .txt files.

Tyyrell_history <- read_tsv("voterhistory.txt")
```
]

.panel[.panel-name[Output]

```{r echo=FALSE}
kbl(head(Tyyrell_history)) %>%
  kable_paper() %>%
  scroll_box(width = "800px", height = "350px")
```
]
]

---

class: center, middle

.bg-washed-green.b--dark-green.ba.bw2.br3.shadow-5.ph4.mt5[
Both `read_csv` and `read_tsv` are special implementations of the more general `read_delim` (meaning read delimited). With this function, you can specify the **delim**iter with the `delim` option allowing you to read in other kinds of delimited data.
] 

---

# How to read-in a xlsx
.panelset[
.panel[.panel-name[Code]

```{r message=FALSE, warning=FALSE}
#Remember, sometimes TSVs (and CSVs--and PSVs for that matter) are written as .txt files.

gainesville_contributions <- read_xlsx("gainesville_pacs.xlsx")
```
]

.panel[.panel-name[Output]

```{r echo=FALSE,}
kbl(head(gainesville_contributions)) %>%
  kable_paper() %>%
  scroll_box(width = "800px", height = "350px")

```
]
]


---

# How to get Census data
.panelset[
.panel[.panel-name[CPS]

```{r fig.align='center', out.width="50%", echo=FALSE}
knitr::include_graphics("cps.png")
```
]

.panel[.panel-name[Other Data]

#data.census.gov
```{r fig.align='center', out.width="50%", echo=FALSE}
knitr::include_graphics("acs and more.png")
```
]
]


---
# Data directly

```{r, eval = FALSE}
#File URL
url1 <- "https://s3.amazonaws.com/dl.ncsbe.gov/data/ncvoter89.zip"

#Downloads file from url1 and pastes it in the current directory as voterfile.zip
download.file(url1, destfile = paste0(getwd(),"/voterfile.zip"))  

#Unzips the file
unzip("voterfile.zip")

#Renames (not needed, but useful for clarity)
file.rename(from = "ncvoter89.txt", to = "voterregistration.txt")

```

---
# Scraping from an API

.panelset[
.panel[.panel-name[Code]

Let's 

```{r message=FALSE, warning=FALSE}

#API web address
abs_voting <- "https://api.gdeltproject.org/api/v2/tv/tv?query=%22absentee%20voting%22%20market:%22National%22&context=%22mail%22&mode=timelinevol&format=csv&datanorm=perc&last24=yes&TIMESPAN=90days"

data_raw <- httr::GET(abs_voting)
abs_mentions <- httr::content(data_raw)

#Renaming columns; filtering to just get CNN, MSNBC, and FOX
abs_mentions <- abs_mentions %>% 
  rename("time" = 1, "channel" = 2) %>% 
  filter(channel %in% c("CNN", "MSNBC", "FOXNEWS"))


```
]

.panel[.panel-name[How it works]
[Documentation](https://blog.gdeltproject.org/gdelt-2-0-television-api-debuts/)

]

.panel[.panel-name[Output]



```{r echo=FALSE, fig.align='center', out.width="50%"}

ggplot(data=abs_mentions, aes(x = time, y = Value)) + geom_line() + facet_wrap(~channel)


```
]
]


---
# Scrape a table from wikipedia

.panelset[
.panel[.panel-name[Initial Scrape]
```{r}
url <- "https://en.wikipedia.org/wiki/2018_United_States_House_of_Representatives_elections_in_Florida"

fl_web_raw <- url %>%
  read_html() %>%
  html_node(xpath = '/html/body/div[3]/div[3]/div[5]/div[1]/table[3]') %>%
  html_table(fill=TRUE)


```
]
.panel[.panel-name[Raw Results]

```{r, echo=FALSE}
kbl(fl_web_raw) %>%
  kable_paper() %>%
  scroll_box(width = "800px", height = "350px")
```


]
.panel[.panel-name[Cleaning]

```{r}
fl_clean <- fl_web_raw

#New Column Names
newcols <- c("District", "R_Votes","R_Per", "D_Votes", "D_Per", "Other_Votes", "Other_Per", "Total_Votes", "Total_Per", "Results")

#Assigning new column names
colnames(fl_clean) <- newcols

#Removing first two rows
fl_clean<-fl_clean[-c(1:2,30),]

#Cleaning Functions
rep_dash <- function(x){ifelse(grepl("â€“",x),NA, x)}
fixnum <- function(x){
  x <- sub("%","",x)
  x <- sub(",","", x)
  x <- as.numeric(x)}

#Final output
fl_clean_final <- fl_clean %>%
  mutate(across(everything(),rep_dash)) %>%
  mutate(across(-matches(c("District","Results")), fixnum))

```
]
.panel[.panel-name[Results]
```{r, echo=FALSE}
kbl(fl_clean_final) %>%
  kable_paper() %>%
  scroll_box(width = "800px", height = "350px")
```
]
]
